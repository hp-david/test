# Interactive ORPO Fine-Tuning & Inference Hub for Open LLMs

<div align="center">

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg?logo=python)
![Jupyter](https://img.shields.io/badge/Jupyter-supported-orange.svg?logo=jupyter)
![HuggingFace](https://img.shields.io/badge/Hugging--Face-model-yellow.svg?logo=huggingface)
![PyTorch](https://img.shields.io/badge/PyTorch-used-ee4c2c.svg?logo=pytorch)
![ORPO](https://img.shields.io/badge/ORPO-fine--tuning-lightblue.svg)

</div>

## üìö Contents

- [üß† Overview](#overview)
- [üóÇ Project Structure](#project-structure)
- [‚öôÔ∏è Setup](#setup)
- [üöÄ Usage](#usage)
- [üìû Contact and Support](#contact-and-support)

---

## Overview

This project demonstrates a full-stack LLM fine-tuning experiment using ORPO (Open-Source Reinforcement Pretraining Objective) to align a base language model with human preference data. It leverages the **Z by HP AI Studio Local GenAI environment**, and uses models such as LLaMA 3, Gemma 1B, and Mistral 7B as foundations.

We incorporate:

- **TensorBoard** for human feedback visualization before fine-tuning
- A flexible model selector and inference runner architecture
- A comparative setup to benchmark base vs fine-tuned models on the same prompts
- Detailed model comparison tools for quality evaluation

---

## Project Structure

```
‚îú‚îÄ‚îÄ config                                         # Configuration files
‚îÇ   ‚îú‚îÄ‚îÄ default_config_cpu.yaml
‚îÇ   ‚îú‚îÄ‚îÄ default_config_multi-gpu.yaml
‚îÇ   ‚îî‚îÄ‚îÄ default_config_one-gpu.yaml
‚îÇ
‚îú‚îÄ‚îÄ core                                           # Core Python modules
‚îÇ   ‚îú‚îÄ‚îÄ comparer
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ model_comparer.py
‚îÇ   ‚îú‚îÄ‚îÄ data_visualizer
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ feedback_visualizer.py
‚îÇ   ‚îú‚îÄ‚îÄ deploy
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ deploy_fine_tuning.py
‚îÇ   ‚îú‚îÄ‚îÄ finetuning_inference
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ inference_runner.py
‚îÇ   ‚îú‚îÄ‚îÄ local_inference
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ inference.py
‚îÇ   ‚îú‚îÄ‚îÄ ggml_convert
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ convert-lora-to-ggml.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ convert.py
‚îÇ   ‚îú‚îÄ‚îÄ selection
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ model_selection.py
‚îÇ   ‚îú‚îÄ‚îÄ target_mapper
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ lora_target_mapper.py
‚îÇ
‚îú‚îÄ‚îÄ notebooks
‚îÇ   ‚îî‚îÄ‚îÄ fine_tuning_orpo.ipynb                   # Main notebook for the project
‚îÇ
‚îú‚îÄ‚îÄ README.md                                    # Project documentation
‚îî‚îÄ‚îÄ requirements.txt                             # Required dependencies

```

---

## Setup

### Step 0: Minimum Hardware Requirements

To ensure smooth execution and reliable model deployment, make sure your system meets the following **minimum hardware specifications** based on the selected model and task (inference or fine-tuning):

### ‚úÖ Model Hardware Matrix

| **Model**                             | **Task**    | **Min VRAM** | **Min RAM** | **GPU Recommendation**           |
| ------------------------------------- | ----------- | ------------ | ----------- | -------------------------------- |
| `mistralai/Mistral-7B-Instruct-v0.1`  | Inference   | 12 GB        | 32 GB       | RTX 3080, A100 (for 4-bit QLoRA) |
|                                       | Fine-tuning | 40‚Äì48+ GB    | 64+ GB      | RTX 4090, A100, H100             |
| `meta-llama/Llama-2-7b-chat-hf`       | Inference   | 6 GB         | 32 GB       | RTX 3080 or better               |
|                                       | Fine-tuning | 40‚Äì48+ GB    | 64+ GB      | RTX 4090+                        |
| `meta-llama/Meta-Llama-3-8B-Instruct` | Inference   | 16 GB        | 32 GB       | RTX 3090, 4090                   |
|                                       | Fine-tuning | 64+ GB       | 64‚Äì96 GB    | Dual RTX 4090 or A100            |
| `google/gemma-7b-it`                  | Inference   | 12 GB        | 32 GB       | RTX 3080 or better               |
|                                       | Fine-tuning | 40+ GB       | 64 GB       | RTX 4090                         |
| `google/gemma-3-1b-it`                | Inference   | 8 GB         | 16‚Äì24 GB    | RTX 3060 or better               |
|                                       | Fine-tuning | 16‚Äì24 GB     | 32‚Äì48 GB    | RTX 3080 / 3090                  |

> ‚ö†Ô∏è These recommendations are based on community benchmarks and documentation provided by Hugging Face, Unsloth, and Google. For production workloads, always monitor VRAM/RAM usage on your system.

### Step 1: Create an AI Studio Project

- Create a new project in [Z by HP AI Studio](https://zdocs.datascience.hp.com/docs/aistudio/overview).

### Step 2: Set Up a Workspace

- Choose **Local GenAI** as the base image.

### Step 3: Clone the Repository

```bash
git clone https://github.com/HPInc/AI-Blueprints.git

```

- Ensure all files are available after workspace creation.

### Step 4: Configure Secrets and Paths

- Add your API keys to the `secrets.yaml` file located in the `configs` folder:
  - `HUGGINGFACE_API_KEY`: Required to use Hugging Face-hosted models instead of a local LLaMA model.
- Edit `config.yaml` with relevant configuration details.

---

## Usage

### Step 1: Run the Notebook

Execute the notebook inside the `notebooks` folder:

```bash
notebooks/fine_tuning_orpo.ipynb
```

This will:

- Select and download a compatible model from Hugging Face
- Apply QLoRA configuration and prepare the model for training
- Run the fine-tuning using ORPO
- Perform evaluation and comparison between the base and fine-tuned models
- Register and serve both base and fine-tuned models via MLflow

### Step 2: Deploy the Chatbot Service

- Go to **Deployments > New Service** in AI Studio.
- Name the service and select the registered model.
- Choose a model version and enable **GPU acceleration**.
- Start the deployment.
- Once deployed, access the **Swagger UI** via the Service URL.

---

## Contact and Support

- Issues & Bugs: Open a new issue in our [**AI-Blueprints GitHub repo**](https://github.com/HPInc/AI-Blueprints).

- Docs: [**AI Studio Documentation**](https://zdocs.datascience.hp.com/docs/aistudio/overview).

- Community: Join the [**HP AI Creator Community**](https://community.datascience.hp.com/) for questions and help.

---

> Built with ‚ù§Ô∏è using [**Z by HP AI Studio**](https://www.hp.com/us-en/workstations/ai-studio.html).
